{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232720da",
   "metadata": {},
   "source": [
    "### The purpose of this analysis is to test whether the refined catalogues are better.\n",
    "\n",
    "Selection of catalogues which appeared to improve. Reference 08-10 analyses notebooks\n",
    "- NC_014328.1.region003\n",
    "- NZ_CP053893.1.region004\n",
    "- NZ_LT906470.1.region002\n",
    "- ranthipeptide_alone\n",
    "\n",
    "### Goal:\n",
    "Parwise test of both accuracy and precision for the MAG_init and MAG_best.\n",
    "\n",
    "We wish to test the null hypotheses:\n",
    "\n",
    "N0.1\n",
    "std(MAG_init:RE) == std(MAG_best:RE)\n",
    "\n",
    "N0.2\n",
    "(MAG_init:RE) == (MAG_best:RE)\n",
    "\n",
    "With in both cases the alternative being smaller std for MAG_best and hopefully also a smaller RE.\n",
    "\n",
    "Here we note that a smaller RE isn't necessarily a requirement for us to percieve the MAG_best to have improve the estimates as the reference needed to calculate RE can be discucssed thus changing the RE-value BUT NOT the spread.\n",
    "\n",
    "\n",
    "### Methods.\n",
    "\n",
    "We will use a paired students t-test to compare distribution of RE to determine if they are different. We use paired t-test as it acts similar to a blocked test - we are comparing results accross widely different dataset sizes which may impact the test result.\n",
    "\n",
    "Alternative:  \n",
    "We could also set it up as following:\n",
    "variable1 = dataset size (0.01 - 0.05)\n",
    "variable2 = method [MAG_init or MAG_best] \n",
    "This will a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30971259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Sequence\n",
    "import configparser\n",
    "import json\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f9b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_subset = [\n",
    "    \"NC_014328.1.region003\",\n",
    "    \"NZ_CP053893.1.region004\",\n",
    "    \"NZ_LT906470.1.region002\",\n",
    "    \"ranthipeptide_alone\"\n",
    "]\n",
    "\n",
    "workdir = os.environ.get(\"ScreenerNBWD\", \"../data/simulated_data_init\")\n",
    "\n",
    "WD_DATA = Path(workdir)\n",
    "\n",
    "\n",
    "Path(\"../data/simulated_data_init\")\n",
    "\n",
    "\n",
    "dir_count_matrices = WD_DATA / \"kmer_quantification/count_matrices\"\n",
    "dir_mag_flat    = WD_DATA / \"MAGinator/screened_flat\"\n",
    "files_mag_gene_sets = dir_mag_flat.glob(\"*_kmers.csv\")\n",
    "fp_simulation_overview = WD_DATA / \"camisim/simulation_overview_full.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f217ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect counts:\n",
    "catalouge_count_files = [file for file in dir_count_matrices.glob(\"*.tsv\") if not file.stem==\"counts_all\"]\n",
    "\n",
    "\n",
    "df_count = pd.concat(\n",
    "    pd.read_csv(file, sep=\"\\t\", index_col=0)\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={'index':'kmer'})\\\n",
    "        .melt(id_vars=[\"kmer\"], var_name='dataset_sample', value_name='count')\\\n",
    "        .assign(catalogue_name = file.stem)\n",
    "    for file in catalouge_count_files\n",
    ").reset_index(drop=True)\n",
    "df_count[[\"dataset\",\"sample\"]] = df_count['dataset_sample'].str.split(\".\", expand=True)\n",
    "df_count.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ff9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect genesets\n",
    "\n",
    "df_genes_sets = pd.concat(\n",
    "    pd.read_csv(file)\\\n",
    "        .assign(catalogue_name = file.stem.rsplit(\"_kmers\",1)[0])\n",
    "    for file in files_mag_gene_sets\n",
    ")\n",
    "df_genes_sets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ea791",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmerset_long = df_genes_sets.loc[df_genes_sets.catalogue_name.isin(catalogue_subset),:]\\\n",
    "    .melt(id_vars = [\"catalogue_name\"], value_vars = [\"init\", \"best\"], var_name=\"method\",value_name=\"kmer\")\n",
    "df_kmerset_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35798c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.merge(df_kmerset_long, df_count, how=\"outer\", on=[\"kmer\", 'catalogue_name'])\n",
    "print(df_combined.__len__())\n",
    "df_combined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fbd2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "def negbinom_mu(column:pd.Series):\n",
    "    nb_fit = sm.NegativeBinomial(column, np.ones_like(column)).fit(disp=0, start_params=[1,1]) #disp=0 == quiet\n",
    "    nb_param_mu = np.exp(nb_fit.params.const)\n",
    "    return nb_param_mu\n",
    "#df_count.loc[df_genes[ref_type]].agg([negbinom_mu, np.median]).T.reset_index()\n",
    "#df_agg = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17332b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estimates = df_combined\\\n",
    "    .groupby(['catalogue_name', 'method','dataset_sample', 'dataset', 'sample'])['count']\\\n",
    "    .agg([negbinom_mu, np.median])\\\n",
    "    .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be7d5f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_estimates.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_simulation = #get_simulation_overview(\"../data/simulated_data/camisim/*GB/simulation_overview.csv\")\n",
    "df_simulation = pd.read_csv(fp_simulation_overview, sep=\"\\t\")\n",
    "df_simulation[\"readsGB\"]  = df_simulation.dataset.str.replace(\"_\",\".\").str.rstrip(\"GB\").astype(float)\n",
    "\n",
    "\n",
    "file_catalogue_grouping = Path(\"../data/simulated_data/catalogues/family_dump.json\")\n",
    "catalogue_groupings     = json.loads(file_catalogue_grouping.read_text())\n",
    "\n",
    "by_sample_grouped = df_simulation.groupby([\"dataset\", \"sample\"])\n",
    "rows = []\n",
    "for name, df_g in by_sample_grouped:\n",
    "    group_rows = [\n",
    "        name + (cat, df_g.loc[df_g.ncbi.isin([member.rsplit(\".\",1)[0] for member in cat_members]),'expected_average_coverage'].sum())\n",
    "        for cat, cat_members in catalogue_groupings.items()]\n",
    "    rows.extend(group_rows)\n",
    "df_catalogue_expect = pd.DataFrame(rows, columns = [\"dataset\", \"sample\",\"catalogue_name\", \"expected_average_coverage\"])\n",
    "df_catalogue_expect.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to counts data\n",
    "df_combined_error = df_estimates.merge(df_catalogue_expect, how=\"left\", on=[\"dataset\", \"sample\",\"catalogue_name\"])\n",
    "df_combined_error.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_error_long = df_combined_error\\\n",
    "    .melt(\n",
    "        id_vars = [\"catalogue_name\",\"method\", \"dataset\",\"sample\", \"expected_average_coverage\"],\n",
    "        value_vars = [\"negbinom_mu\", \"median\"], \n",
    "        value_name = \"estimate\",\n",
    "        var_name = \"estimate_agg\")\n",
    "df_combined_error_long.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808db765",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_error_long[\"RE\"] = (df_combined_error_long.estimate\n",
    " - df_combined_error_long.expected_average_coverage) / df_combined_error_long.expected_average_coverage\n",
    "df_combined_error_long[\"RAE\"] = df_combined_error_long[\"RE\"].abs()\n",
    "df_combined_error_long.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3826f7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_groups = df_combined_error_long.catalogue_name.drop_duplicates().count()\n",
    "n_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a679de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.colors.qualitative.Plotly\n",
    "color_map = {\n",
    "    'init':px.colors.qualitative.Plotly[0],\n",
    "    'best':px.colors.qualitative.Plotly[1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nb = df_combined_error_long.query(\"estimate_agg == 'negbinom_mu'\")\n",
    "df_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ff45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "catagories, df_catagories = zip(*list(df_nb.groupby(\"catalogue_name\")))\n",
    "titles = []\n",
    "for cat in catagories:\n",
    "    titles.extend([cat, \"Summarised\"])\n",
    "titles.extend([\"Combined catagories\", \"Total\"])\n",
    "# Create figure\n",
    "fig_comp = make_subplots(\n",
    "    rows=5, cols=2,\n",
    "    shared_yaxes=True,\n",
    "    column_widths = [0.7, 0.3],\n",
    "    subplot_titles=titles,\n",
    "    vertical_spacing=0.06,\n",
    "    horizontal_spacing=0.03\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for cat_i, (cat_name, df_group_cat) in enumerate(zip(catagories, df_catagories)):\n",
    "    \n",
    "    for method, df_method in df_group_cat.groupby(\"method\"):\n",
    "        df_method.sort_values(\"dataset\", inplace=True)\n",
    "        \n",
    "        fig_comp.add_trace(\n",
    "            go.Box(\n",
    "                x = df_method[\"dataset\"].values,\n",
    "                y = df_method[\"RE\"].values,\n",
    "                fillcolor = 'rgba(255,255,255,0)', #hide box\n",
    "                legendgroup = method,\n",
    "                name = method,\n",
    "                line = {'color': 'rgba(255,255,255,0)'}, #hide box\n",
    "                marker = {'color': color_map[method]},\n",
    "                offsetgroup = method,\n",
    "                orientation = 'v',\n",
    "                pointpos = 0,\n",
    "                jitter=0.3,\n",
    "                alignmentgroup = 'True',\n",
    "                boxpoints = 'all',\n",
    "                showlegend = cat_i == 0,\n",
    "                boxmean='sd'\n",
    "            ),\n",
    "            row=cat_i+1, col=1\n",
    "        )\n",
    "        \n",
    "        fig_comp.add_trace(\n",
    "            go.Box(\n",
    "                x = [\"Across datasets\" for i in range(len(df_method))],\n",
    "                y = df_method[\"RE\"].values,\n",
    "                fillcolor = 'rgba(255,255,255,0)', #hide box\n",
    "                legendgroup = method,\n",
    "                name = method,\n",
    "                line = {'color': 'rgba(255,255,255,0)'}, #hide box\n",
    "                marker = {'color': color_map[method]},\n",
    "                offsetgroup = method,\n",
    "                orientation = 'v',\n",
    "                pointpos = 0,\n",
    "                jitter=0.5,\n",
    "                alignmentgroup = True,\n",
    "                boxpoints = 'all',\n",
    "                showlegend = False,\n",
    "                #boxmean='sd'\n",
    "            ),\n",
    "            row=cat_i+1, col=2\n",
    "        )\n",
    "# add final summarising row.\n",
    "for method, df_method in df_nb.groupby(\"method\"):\n",
    "        df_method.sort_values(\"dataset\", inplace=True)\n",
    "        \n",
    "        fig_comp.add_trace(\n",
    "            go.Box(\n",
    "                x = df_method[\"dataset\"].values,\n",
    "                y = df_method[\"RE\"].values,\n",
    "                fillcolor = 'rgba(255,255,255,0)', #hide box\n",
    "                legendgroup = method,\n",
    "                name = method,\n",
    "                line = {'color': 'rgba(255,255,255,0)'}, #hide box\n",
    "                marker = {'color': color_map[method]},\n",
    "                offsetgroup = method,\n",
    "                orientation = 'v',\n",
    "                pointpos = 0,\n",
    "                jitter=0.3,\n",
    "                alignmentgroup = 'True',\n",
    "                boxpoints = 'all',\n",
    "                showlegend = cat_i == 0,\n",
    "                boxmean='sd'\n",
    "            ),\n",
    "            row=cat_i+2, col=1\n",
    "        )\n",
    "        \n",
    "        fig_comp.add_trace(\n",
    "            go.Box(\n",
    "                x = [\"Across datasets\" for i in range(len(df_method))],\n",
    "                y = df_method[\"RE\"].values,\n",
    "                fillcolor = 'rgba(255,255,255,0)', #hide box\n",
    "                legendgroup = method,\n",
    "                name = method,\n",
    "                line = {'color': 'rgba(255,255,255,0)'}, #hide box\n",
    "                marker = {'color': color_map[method]},\n",
    "                offsetgroup = method,\n",
    "                orientation = 'v',\n",
    "                pointpos = 0,\n",
    "                jitter=0.5,\n",
    "                alignmentgroup = True,\n",
    "                boxpoints = 'all',\n",
    "                showlegend = False,\n",
    "                #boxmean='sd'\n",
    "            ),\n",
    "            row=cat_i+2, col=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea67f38",
   "metadata": {},
   "source": [
    "### Initial evalutations\n",
    "\n",
    "From the plot we can get a general feel that there is large spread for <=0.01GB for both best and initial.\n",
    "\n",
    "However, we also note that the MAG_best method appears to have slight more tightly grouped estimates.\n",
    "\n",
    "This is however sadly not a clear image, we do note that for the second row we observe a large difference - this could however be due to a very poor seed.\n",
    "\n",
    "If we combine each row (catalogue) we do not observe any gain in precision, and neither if we total across catalogues and datasets (bottom right). However, if we look individually at datasets within each catalogue we generally observe a smaller spread (better) for the refined set ('best'). This trend also holds when we collapse the catalogues (bottom left).\n",
    "\n",
    "Thus, we have some tentative evidence that the refinement methods provides more stable (higher precision) estimates compared to the initial seed when looking at the datasets >= 0.02GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64791f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_comp.update_layout(height=1000, boxmode='group', title=\"Overview of Relative error\")\n",
    "fig_comp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eca8f7",
   "metadata": {},
   "source": [
    "### Statistical test\n",
    "\n",
    "Perform Fligner-Killeen test for equality of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e461a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nb.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea271e",
   "metadata": {},
   "source": [
    "#### Test without any blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f5e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates_best = df_nb.query(\"method == 'best'\").RE.values.tolist()\n",
    "estimates_init = df_nb.query(\"method == 'init'\").RE.values.tolist()\n",
    "\n",
    "stat, p = fligner(estimates_best, estimates_init)\n",
    "\n",
    "print(f\"\"\"\n",
    "Fligner-Killeen test for equality of variance. When viewing the total population (across datasets and catalogues.)\n",
    "\n",
    "Variance:\n",
    "init: {np.var(estimates_init, ddof=1):.2f}\n",
    "best: {np.var(estimates_best, ddof=1):.2f}\n",
    "\n",
    "The probability of the two populations of equal variance to give rise to an equal or\n",
    "more extreme difference in variance is given by p={p:.1e} with a statistic of ({stat:.3f}).\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68936c59",
   "metadata": {},
   "source": [
    "The Fligner killeen suggests that the variance between the two are not equal and given the variance we actually find that overall the variance is larger fo the MAG_best. This is however across both catalouges and datasets and may not be good estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989423ff",
   "metadata": {},
   "source": [
    "### Test when blocking for both Dataset and Catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26267d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nb_wide = df_nb[[\"catalogue_name\",\"method\",\"dataset\",\"sample\",\"estimate\",\"RE\",\"RAE\"]]\n",
    "df_nb_wide.pivot(index=[\"catalogue_name\",\"dataset\",\"sample\"], columns='method', ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set = df_nb.query(\"catalogue_name == 'NC_014328.1.region003' & dataset == '0_005GB'\")\n",
    "df_set.head(3)\n",
    "#df_set.pivot(columns = [\"method\",\"RE\",\"RAE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f9ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rows = []\n",
    "for (cat, db), df_sub_data in df_nb.groupby([\"catalogue_name\", \"dataset\"]):\n",
    "    estimates_best = df_sub_data.query(\"method == 'best'\").RE.values.tolist()\n",
    "    estimates_init = df_sub_data.query(\"method == 'init'\").RE.values.tolist()\n",
    "    \n",
    "    stat, p = fligner(estimates_best, estimates_init)\n",
    "    \n",
    "    result_rows.append({\n",
    "        'catalogue_name': cat,\n",
    "        'dataset': db,\n",
    "        'variance_init' : np.var(estimates_init),\n",
    "        'variance_best' : np.var(estimates_best),\n",
    "        'statistic' : stat,\n",
    "        'p-value' : p,\n",
    "        \"log2Fold\": np.log2(np.var(estimates_init)) - np.log2(np.var(estimates_best))\n",
    "    })\n",
    "    \n",
    "df_results = pd.DataFrame(result_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a2192e",
   "metadata": {},
   "source": [
    "### From the test of individual catalogue/dataset pairs we find no significant difference.\n",
    "\n",
    "We note however that the statistic reach a global maximum of 2.51... which could be caused by the sample sizes (5) being too small. We therefore try gain while blocking only catalogue and not dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c53b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values(\"p-value\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eedc569",
   "metadata": {},
   "source": [
    "### Blocking for catalogues\n",
    "Testing within catalogue best vs within catalogue init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rows2 = []\n",
    "for cat, df_sub_data in df_nb.groupby([\"catalogue_name\"]):\n",
    "    estimates_best = df_sub_data.query(\"method == 'best'\").RE.values.tolist()\n",
    "    estimates_init = df_sub_data.query(\"method == 'init'\").RE.values.tolist()\n",
    "    \n",
    "    stat, p = fligner(estimates_best, estimates_init)\n",
    "    \n",
    "    result_rows2.append({\n",
    "        'catalogue_name': cat,\n",
    "        #'dataset': db,\n",
    "        'variance_init' : np.var(estimates_init),\n",
    "        'variance_best' : np.var(estimates_best),\n",
    "        'statistic' : stat,\n",
    "        'p-value' : p,\n",
    "        \"log2Fold\": np.log2(np.var(estimates_init)) - np.log2(np.var(estimates_best))\n",
    "    })\n",
    "    \n",
    "df_results2 = pd.DataFrame(result_rows2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed0a92",
   "metadata": {},
   "source": [
    "### Results from blocking only catalogue:\n",
    "\n",
    "Here we note that the variance is consistently larger for the variance best compared to the inital.\n",
    "We note that this is only significant for __NZ_LT906470.1.region002__ And that this tentative conclusion is without taking into account multiple testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf3e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results2.sort_values(\"p-value\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rows3 = []\n",
    "for cat, df_sub_data in df_nb.groupby([\"dataset\"]):\n",
    "    estimates_best = df_sub_data.query(\"method == 'best'\").RE.values.tolist()\n",
    "    estimates_init = df_sub_data.query(\"method == 'init'\").RE.values.tolist()\n",
    "    \n",
    "    stat, p = fligner(estimates_best, estimates_init)\n",
    "    \n",
    "    result_rows3.append({\n",
    "        'dataset': cat,\n",
    "        #'dataset': db,\n",
    "        'variance_init' : np.var(estimates_init),\n",
    "        'variance_best' : np.var(estimates_best),\n",
    "        'statistic' : stat,\n",
    "        'p-value' : p\n",
    "    })\n",
    "    \n",
    "df_results3 = pd.DataFrame(result_rows3)\n",
    "df_results3[\"log2Fold\"] = np.log2(df_results3.variance_init / df_results3.variance_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d7651",
   "metadata": {},
   "source": [
    "### When blocking for dataset\n",
    "We see that for all datasets the varaince are multiple folds lower for variance_best compared to variance_init.\n",
    "\n",
    "These observations also appears to be significant.\n",
    "\n",
    "Thus, these gives some evidence that there could be value (as in better precision) for the refined gene-set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ecb354",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results3.sort_values(\"log2Fold\",ascending=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb00b4b5",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "I the investigations both exploratory and statistical show tentative evidence that the refinement method results in more consistent (precise) results. \n",
    "\n",
    "I believe that the reason why we observe init to have lesser variance when not looking into within dataset analysis is that the variance is much larger in the smallest datasets which the drive the analysis. Not entirely unlike the learnings in the Simpson Paradox.\n",
    "\n",
    "When we look within each dataset we do observe that best > init in terms of better precision.\n",
    "\n",
    "However, we also realize that perhaps the nature of our initial dataset is to small and simple to properly investigate whether the NB catalogue refinement method works. We are simply tring to optimize on a too small area.\n",
    "\n",
    "We therefore decided to scale up and work on a much large simulated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac7c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c91d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
